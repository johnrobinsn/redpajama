{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RedPajama (\"OpenLlama\")\n",
    "by John Robinson 05/15/2023 [Follow @johnrobinsn on Twitter](https://twitter.com/johnrobinsn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.storminthecastle.com/img/github.svg\">](https://github.com/johnrobinsn/redpajama/blob/main/finetune_redpajama_clean.ipynb) [<img src=\"https://www.storminthecastle.com/img/colab.svg\">](https://colab.research.google.com/github/johnrobinsn/edpajama/blob/main/finetune_redpajama_clean.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details please see the [blog article](https://www.storminthecastle.com/posts/finetune_redpajama/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Configure the base model and a few other variables that we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '3B' #'7B' # Pick your poison\n",
    "\n",
    "if model == '7B':\n",
    "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\",\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\")\n",
    "    run_name = 'redpj7B-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'redpj7B-lora-int8-alpaca'\n",
    "    output_dir = 'redpj7B-lora-int8-alpaca-results'\n",
    "else: #3B\n",
    "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\n",
    "    run_name = 'redpj3B-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'redpj3B-lora-int8-alpaca'\n",
    "    output_dir = 'redpj3B-lora-int8-alpaca-results'\n",
    "\n",
    "model_name[1],dataset,peft_name,run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    !pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
    "    !pip install -Uqq \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "    !pip install -Uqq wandb\n",
    "    !pip install -Uqq protobuf==3.20\n",
    "\n",
    "# uncomment the following line to install the required dependencies\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: If you just want to do inference you can jump all the way down to the [\"Evaluate\"](#evaluate) cell and start running from there to download my adapter weights from HF hub and try some prompts through the finetuned model.__\n",
    "\n",
    "But if you want to train keep going... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tracking and Monitoring using Weights and Biases\n",
    "\n",
    "This notebook has support for logging the training run to [weights and biases (wandb)](https://wandb.ai/site).  This makes it very easy to track, monitor and annotate your training sessions from anywhere.  \n",
    "\n",
    "Run the next cell and follow the directions given to authenticate with wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_to = \"wandb\" # \"none\"\n",
    "\n",
    "if report_to != \"none\":\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After authenticating, we have to initialize wandb.  We add a few key-value pairs about the run to the information that will be logged to the wandb dashboard.  \n",
    "\n",
    "_Note: You can add more key/values if you'd like._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=run_name,config={\n",
    "    \"model\": model_name[1],\n",
    "    \"dataset\":dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you get training started below.  You can revisit the wandb links shown above to monitor the status of your training run from anywhere with Internet connectivity.  \n",
    "\n",
    "_Note: I like to send the link (View run) to my phone so that I can monitor on the go..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "The tokenizer converts words into a list/tensor of numbers so that the model can process them.  Each language model has been trained using a specific tokenizer.  If your base model is already supported by huggingface then the transformer library makes it very easy to load the correct tokenizer for your given model.  Just use the AutoTokenizer class to create an instance of the correct tokenizer by just specifying the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Loading tokenizer for model: \", model_name[1])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[1],add_eos_token=True)\n",
    "tokenizer.pad_token_id = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that I've found with many of the finetuning scripts and notebooks found online is that the \"end-of-stream\" handling is not done correctly, so in many cases the finetuned models don't know when to stop emitting tokens and tend to \"blabber\" on.  Since we are finetuning on an instruction following task, we would like the model to respond to the instruction prompt succintly and then stop.  There are a number of ways to approach this, but the way I approach it here is to explicitly add a new token to represent end-of-stream, &lt;eos&gt; and use that eos token during training to teach the model when it should stop. Then during inference, we can use that token to recognize when the model is done responding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
    "print('eos_token_id:',tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data in the alpaca dataset\n",
    "\n",
    "def tokenize(prompt, tokenizer,add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt+\"<eos>\",  # add the end-of-stream token\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"],\n",
    "        \"attention_mask\": result[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a quick try and note the <eos> token id at the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('hi there<eos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "When finetuning your model the dataset that you chose has to be aligned with your downstream task. We're using popular Instruction Following Dataset, called Alpaca. For convenience, I have a copy of the alpaca dataset that has been cleaned published on the HuggingFace hub. We can just download it and access it from cache using the load_dataset API shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "data = load_dataset(dataset)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dataset consists of 51,942 rows with the following features ['instruction','input','output'].  Let's take a look at one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see an item that includes an 'instruction' to direct our model.  An optional 'input' which provides context to the instruction.  And then an expected output for the model.\n",
    "\n",
    "Our goal in finetuning our model is to use this dataset to train our model to \"behave\" in a similar way.  Given an instruction respond with an appropriate response generalizing to the knowledge already encoded in the base model.\n",
    "\n",
    "But we can't directly use this JSON object to train our model.  Our model can only process an ordered sequence of tokens that represent words.  So we use a \"prompt template\" to convert each of these JSON objects in our dataset into a sequence of words.  The prompt template follows a consistent pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what what our example looks like when \"templatized\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_prompt(data['train'][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact wording of the template is somewhat arbitrary.  It's more of a consistent pattern that after training will drive the model into responding similarly when exposed to a similar prompt.  You should be able to pick out the \"instruction\", \"input\", and \"output\" from the example.  \n",
    "\n",
    "It is important that the output from the dataset is at the end of templatized prompt, since at inference time we will only provide the prompt up to **but not including the output**.  We'll expect our model to respond to our instruction on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split out a validation dataset from our training dataset so that we can track how well the finetuning process is learning to generalize to unseen prompts and so that we make sure we're only checkpointing our model when the validation loss is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SET_SIZE = 2000  # we set aside 2000 items from our dataset for validation during training\n",
    "\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the training dataset and the validation dataset by running the data through the prompt templating process and then by tokenizing the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n",
    "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Configure the Model for Training\n",
    "\n",
    "Load the specified RedPajama base model from the HuggingFace hub.\n",
    "\n",
    "_Note: Llama, Redpajama and other decoder-only models are supported by the AutoModelForCausalLM class.  But for encoder-decoder models such as the [**google/t5**](https://huggingface.co/google/flan-t5-xxl) models you'll need to use the AutoModelForSeq2SeqLM class and the training details are a little bit different.  Here is a [similar notebook](https://github.com/johnrobinsn/flan_ul2/blob/main/train-peft-flan-ul2-int8-alpaca.ipynb) for finetuning t5* models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading model for model: \", model_name[0])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[0],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare our model for the LoRA int-8 training usingÂ the HF peft library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r= 8, \n",
    " lora_alpha=16,\n",
    " target_modules=[\"query_key_value\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the Lora Adapters into the model notice the significant reduction in the number of trainable paramters.\n",
    "\n",
    "We'll leverage the training loop from the transformers library since it does a pretty good job with handling the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "logging_steps = 20\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=output_dir,\n",
    "        report_to=report_to if report_to else \"none\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Run the training loop.  This will take quite a while on a Titan RTX the 3B will take X and the 7B took Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Adpater Model to Disk\n",
    "\n",
    "Now that we've trained the model we'll want to save our weights.  First I demonstrate how to save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "trainer.model.save_pretrained(peft_name)\n",
    "tokenizer.save_pretrained(peft_name)\n",
    "\n",
    "# if you want to save the base model to disk call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the Trained Adapter Model to the HuggingFace Hub\n",
    "\n",
    "Even better than saving your trained weights to disk you can push them up the HuggingFace Hub.  This makes it super easy to share your trained adapter with others or to setup your model for inference on other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq huggingface_hub\n",
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't already have the git extensions for large file storage you might have to install it now.\n",
    "# Here is how you can do this for Linux from the shell.  For other OSs please refer to the git-lfs documentation.\n",
    "# sudo apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
    "trainer.model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You chould be able to check out HuggingFace and see your LoRA Adapter Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Up Memory\n",
    "\n",
    "Since we likely used a lot of memory during training and we'll need that memory back to try the model out we take a few steps to free up VRAM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "config = None\n",
    "model = None\n",
    "tokenizer=None\n",
    "trainer=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate\n",
    "Here we'll try out the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "#peft_model_id = \"results\"\n",
    "#config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[0],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[1])\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
    "# tokenizer.eos_token_id = 2\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt template we'll use for inference.\n",
    "\n",
    "_Note: It's important that it's identical to one we used for training above, but it omits the \"output/response\" as our model will generate that for us._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small utility function that let's us easily prompt our model with an instructin and an optional input.  It handles templating the prompt, tokenizing the templatized prompt, decoding the result and then finally stripping off the prompt from the response and just leaving us with the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(instruction,input=None,maxTokens=256):\n",
    "    prompt = generate_prompt({'instruction':instruction,'input':input})\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=maxTokens, \n",
    "                             do_sample=True, top_p=0.9,pad_token_id=tokenizer.eos_token_id,\n",
    "                             forced_eos_token_id=tokenizer.eos_token_id)\n",
    "    outputs = outputs[0].tolist()\n",
    "    # Stop decoding when hitting the EOS token\n",
    "    if tokenizer.eos_token_id in outputs:\n",
    "        eos_index = outputs.index(tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(outputs[:eos_index])\n",
    "        # Don't show the prompt template\n",
    "        sentinel = \"### Response:\"\n",
    "        sentinelLoc = decoded.find(sentinel)\n",
    "        if sentinelLoc >= 0:\n",
    "            print(decoded[sentinelLoc+len(sentinel):])\n",
    "        else:\n",
    "            print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
    "    else:\n",
    "        print('Warning: no <eos> detected ignoring output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating using the Base Model\n",
    "\n",
    "This demonstrates the behavior of the RedPajama model with no finetuning applied.  Below I'll load the LoRA adapter that has been trained on the finetuning task to demonstrate the change in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LoRA Adapter\n",
    "\n",
    "As you can see the generated text doesn't seem very responsive to the prompt.  Now let's load the trained LoRA adapter and see what happens.\n",
    "\n",
    "_Note: Here you can either load up my pretrained Lora adapter from HuggingFace hub.  Or if you trained your own adapter above you can uncomment the specified line below to load your adapter from disk._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = f'johnrobinsn/{peft_name}' # By default use my pretrained adapter weights\n",
    "#peft_model_id = peft_name # Uncomment to use locally saved adapter weights if you trained above\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model adapter loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try the same prompt again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few More Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('Who was the first man to walk on the moon and tell me where he was born.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we provide not only an instruction but also provide some context for the instruction which is a list of possible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('Identify the odd one out and explain why.','Twitter, Instagram, Telegram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('Write a poem about about a cat',maxTokens=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4eddb440959a89d6755541b4ddad18aa547193e63575f384aa5d9491d8be2537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
